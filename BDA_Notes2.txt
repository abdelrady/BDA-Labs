||D1|| = sq.root(sum(d1^2) for all d1)


Lesson #9

Absolute support, or, support count of X
Relative support: The fraction of transactions that contain X
Frequent Itemsets: An itemset X is frequent if Xâ€™s support is no less than a predefined minimum support threshold, called minsup.
An association rule has the form R1: X -> Y, where X and Y are itemsets. The rule says "whenever X occurs, we expect to find Y as wellâ€œ.
Support of R1: s(R1) = support(X uinion Y) or probability that a transaction contains X and Y
Confidence of R1: c(R1) = support(X union Y) / support(X), or conditional probability that a transaction having X also contains Y.
The confidence of a rule tells us how reliable the rule is. The closer the value c(R1) comes to 1, the more reliable the rule R1 is.
A strong rule is a rule X -> Y with the following two properties:
	1- support â‰¥ minimum support (minsup) and
	2- confidence â‰¥ minimum confidence (minconf).

The Apriori Algorithm
Two step process:
	1- First, mine all frequent patterns. A frequent pattern is also called a frequent itemset or a large itemset.
		a- join step: Generate all possible candidate itemsets of size k+1 ( we join only if the first k-1
items are identical.)
		b- prune step: Remove those candidates in Ck+1 that cannot be frequent. (Using downward closure property)
	2- Second, mine strong rules from frequent itemsets.
		a- For each frequent itemset I, and for each proper nonempty subset X of I, Let Y = I - X then X -> Y is a strong rule if Confidence(X -> Y) â‰¥ minconf

The apriori property (downward closure property): A subset of a frequent itemset must also be a frequent itemset.

The lift of a rule measures how much more likely one itemset is purchased with another itemset compared to its typical rate of purchase.

(Negative correlation means X and Y occur together less frequently than would happen by chance.)
To see if a rule X->Y indicates that Y is purchased more frequently when X is purchased than otherwise, compare confidence(X->Y) to support(Y). This gives the Lift Formula: ð‘™ð‘–ð‘“ð‘¡ ð‘‹ â†’ ð‘Œ = ð‘ð‘œð‘›ð‘“ð‘–ð‘‘ð‘’ð‘›ð‘ð‘’(ð‘‹â†’ð‘Œ) / ð‘ ð‘¢ð‘ð‘ð‘œð‘Ÿð‘¡(ð‘Œ)
â€¢ lift > 1: positively correlated
â€¢ lift < 1: negatively correlated,
â€¢ lift = 1: independent (no correlation)


---------------------------------------------------------------------------------------------------------------------------
Cluster Analysis = data segmentation

Clustering is a technique used for automatic identification of natural groupings of things

A good clustering method will produce high quality clusters
 * high intra-class similarity: cohesive within clusters
 * low inter-class similarity: distinctive between clusters

Clustering Types
	1- Bayesian (Decision Based, nonparametric)
	2- Hierarchical (Divisive, agglomerative)
	3- Partitional (Centroid or K-Means, Model Based, Graph Theoretic, Spectral)

* Hierarchical algorithms find successive clusters using previously established clusters. These algorithms can be either agglomerative (â€œbottom-upâ€) or divisive (â€œtop-downâ€):
1- Agglomerative algorithms begin with each element as a separate cluster and merge them into successively larger clusters;
2- Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters.

* Partitional algorithms: Given a set of n objects, a partitioning method constructs k partitions of the data, where each partition represents a cluster and k <= n.



sup(x & y) = 362 / (362+49+823+1527)
sup(x) = (362+823) / (362+49+823+1527)
