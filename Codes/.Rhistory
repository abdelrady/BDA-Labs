#examine a particular item(a column of data)
#the proportion of transactions that contain the item
itemFrequency(Groceries[, 1:3])
#plot frequent items with min support = 0.1
itemFrequencyPlot(Groceries, support = 0.3)
#plot top 20 frequent items
itemFrequencyPlot(Groceries, topN = 20)
#use apriori to generate rules
rules <- apriori(Groceries,
parameter = list(support = 0.006, confidence = 0.25, minlen = 2))
summary(rules)
summary(rules)
inspect(rules[1:5])
#use apriori to generate rules
rules <- apriori(Groceries,
parameter = list(support = 0.006, confidence = 0.25, minlen = 3))
summary(rules)
inspect(rules[1:5])
#get top five highest lift rules
inspect(sort(rules, by="lift")[1:5])
#find subset of the rules with berrries appearing in the rule
sub.rules <- subset(rules, items %in% "berries")
inspect(sub.rules)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
iris
head(iris)
#this command initializes R's random number generator to a specific sequence
#so set.seed to ensure reproducibility.
ØŸset.seed
#this command initializes R's random number generator to a specific sequence
#so set.seed to ensure reproducibility.
?set.seed
?kmeans
set.seed(20)
irisCluster <- kmeans(iris[, 3:4], 3)
irisCluster
irisCluster
dim(iris)
irisCluster
irisCluster$centers
irisCluster$cluster
iris[, 3:4]
iris[, 3:4]
irisCluster <- kmeans(iris[, 1:4], 3)
irisCluster
irisCluster <- kmeans(iris[, 1:2], 3)
?kmeans
irisCluster
iris$Species
#how good the clustering result?
table(irisCluster$cluster, iris$Species)
irisCluster <- kmeans(iris[, 3:4], 3)
?kmeans
irisCluster
#how good the clustering result?
table(irisCluster$cluster, iris$Species)
iris[, 3:4]
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
#plot the data to see the clusters
irisCluster$cluster.1 <- as.factor(irisCluster$cluster)
#plot the data to see the clusters
irisCluster$cluster.1 <- as.factor(irisCluster$cluster)
irisCluster$cluster.1
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster.1)) + geom_point()
#about between_SS and total_SS
irisCluster$centers
irisCluster$cluster
irisCluster$totss
irisCluster$withinss
irisCluster <- kmeans(iris[, 1:2], 3)
#how good the clustering result?
table(irisCluster$cluster, iris$Species)
#plot the data to see the clusters
irisCluster$cluster.1 <- as.factor(irisCluster$cluster)
irisCluster$cluster.1
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster.1)) + geom_point()
#about between_SS and total_SS
irisCluster$centers
irisCluster$cluster
irisCluster$totss
irisCluster <- kmeans(iris[, 3:4], 3)
?kmeans
irisCluster
#how good the clustering result?
table(irisCluster$cluster, iris$Species)
#plot the data to see the clusters
irisCluster$cluster.1 <- as.factor(irisCluster$cluster)
irisCluster$cluster.1
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster.1)) + geom_point()
#about between_SS and total_SS
irisCluster$centers
irisCluster$cluster
irisCluster$totss
irisCluster$withinss
mean1 <- mean(iris$Petal.Length)
mean2 <- mean(iris$Petal.Width)
#compute total_SS by hand
sum((iris$Petal.Length - mean1)^2 + (iris$Petal.Width - mean2)^2)
#compute within-cluster sum of squares for cluster 1
mean1 <- irisCluster$centers[1,1]
mean2 <- irisCluster$centers[1,2]
iris.cluster1 <- iris[irisCluster$cluster == 1,]
sum((iris.cluster1$Petal.Length - mean1)^2 + (iris.cluster1$Petal.Width - mean2)^2)
library(tm)#Framework for textmining.
library(SnowballC)#Provides wordStem() for stemming.
Doc1.Train.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-train/sci.space",sep=""))
Doc1.Train <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc1.Test.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-test/sci.space",sep=""))
Doc1.Test <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc2.Train.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-train/rec.autos",sep=""))
Doc2.Train <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc2.Test.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-test/rec.autos",sep=""))
Doc2.Test <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
# merge 4 corpus
merge.corpus <- c(Doc1.Train,Doc1.Test,Doc2.Train,Doc2.Test)
merge.corpus
#Preprocessing
# Convert to lower case
merge.corpus.tranf <- tm_map(merge.corpus, content_transformer(tolower))
# Remove Punctuation
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removePunctuation)
# Remove stop words
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removeWords, stopwords("english"))
# specify your stopwords as a character vector
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removeWords, c("since", "let", "yes", "every", "yeah"))
# ignore extremely rare words i.e. terms that appear in less then 1% of the documents
minTermFreq <- 5
dtm = DocumentTermMatrix(merge.corpus,
control = list(
minWordLength = 2, minDocFreq = 5
#wordLengths=c(2, Inf),
#bounds = list(global = c(minTermFreq, Inf))
))
dtm.matrix = as.matrix(dtm)
dtm.matrix[200:230, 20:30]
train.doc <- dtm.matrix[c(1:100,201:300),]
test.doc <- dtm.matrix[c(101:200,301:400),]
Tags <- factor(c(rep("Sci",100), rep("Rec",100)))
dim(train.doc)
dim(test.doc)
length(Tags)
library(class) # Using kNN
prob.test<- knn(train.doc, test.doc, Tags, k = 3, prob=TRUE)
prob.test
# Display Classification Results
a <- c(1:400) #document ids
# Display Classification Results
a <- c(1:length(prob.test)) #document ids
# Display Classification Results
a <- c(1:length(prob.test)) #document ids
# Display Classification Results
a <- c(1:length(prob.test)) #document ids
b <- prob.test #predicts by the algorithm
#or b <- levels(prob.test)[prob.test]
c <- attributes(prob.test)$prob #proportion of the votes for the winning class
d <- prob.test == Tags
result <- data.frame(Doc=a, Predict=b,Prob=c,Correct= d)
result
library(tm)#Framework for textmining.
library(SnowballC)#Provides wordStem() for stemming.
pdf.loc <- file.path(paste(getwd(),"/demoData", sep=""), "I have a dream.pdf")
pdf.loc
#create a uniform resource identifier source
#A uniform resource identifier source interprets each URI as a document.
cname <- URISource(pdf.loc)
cname
corpus.pdf <- Corpus(cname, readerControl=list(reader=readPDF))
inspect(corpus.pdf)
corpus.pdf[[1]]$content #to view the content extracted
corpus.pdf[[1]]$meta
dtm <- DocumentTermMatrix(corpus.pdf) #Create Document Term Matrix
freq <- colSums(as.matrix(dtm)) #Term frequencies
freq
dtm
as.matrix(dtm)
freq
freq
as.matrix(dtm)
ord <- order(freq) #Ordering the frequencies
ord
freq[tail(ord)] #Most frequent terms
findFreqTerms(dtm, lowfreq = 10) #get the terms that appear at least 10 times
getTransformations()
#Preprocessing
# Convert to lower case
#tolower is a function we can wrapped with content_transformer
corpus.tranf <- tm_map(corpus.pdf, content_transformer(tolower))
# Remove Punctuation
corpus.tranf <- tm_map(corpus.tranf, removePunctuation)
# Remove stop words
corpus.tranf <- tm_map(corpus.tranf, removeWords, stopwords("english"))
# Remove stop words
stopwords("english")
stopwords("english")
# Remove stop words
# stopwords("english")
corpus.tranf <- tm_map(corpus.tranf, removeWords, stopwords("english"))
# specify your stopwords as a character vector
corpus.tranf <- tm_map(corpus.tranf, removeWords, c("since", "let"))
dtm <- DocumentTermMatrix(corpus.tranf) #Create Document Term Matrix
# Create a Word Cloud Plot
# install.packages("wordcloud")
library(wordcloud)
wordcloud(names(freq), freq, min.freq=2, colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=5, colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
dtm <- DocumentTermMatrix(corpus.tranf) #Create Document Term Matrix
inspect(dtm[1:1,20:30])
freq <- colSums(as.matrix(dtm)) #Term frequencies
ord <- order(freq) #Ordering the frequencies
freq[tail(ord)] #Most frequent terms
findFreqTerms(dtm, lowfreq = 5) #get the terms that appear at least 5 times
# Create a Word Cloud Plot
# install.packages("wordcloud")
library(wordcloud)
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
# specify your stopwords as a character vector
corpus.tranf <- tm_map(corpus.tranf, removeWords, c("since", "let", "yeah", "yes"))
dtm <- DocumentTermMatrix(corpus.tranf) #Create Document Term Matrix
inspect(dtm[1:1,20:30])
freq <- colSums(as.matrix(dtm)) #Term frequencies
ord <- order(freq) #Ordering the frequencies
freq[tail(ord)] #Most frequent terms
findFreqTerms(dtm, lowfreq = 5) #get the terms that appear at least 5 times
# Create a Word Cloud Plot
# install.packages("wordcloud")
library(wordcloud)
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
#text mining - clustering
dir = DirSource(paste(getwd(),"/demoData/news",sep="")) #read all txt files in news folder of current working directory
corpus = Corpus(dir, readerControl=list(reader=readPlain))
ndocs <- length(corpus)
ndocs
# ignore extremely rare words i.e. terms that appear in less then 1% of the documents
minTermFreq <- ndocs * 0.01
# ignore overly common words i.e. terms that appear in more than 100% of the documents
maxTermFreq <- ndocs * 1.0
dtm = DocumentTermMatrix(corpus,
control = list(
stopwords = TRUE,
wordLengths=c(4, 15),
removePunctuation = T,
removeNumbers = T,
stemming = T,
bounds = list(global = c(minTermFreq, maxTermFreq))
))
dtm <- weightTfIdf(dtm)
dtm.matrix = as.matrix(dtm)
### don't forget to normalize the vectors so Euclidean makes sense
?apply
norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
m_norm <- norm_eucl(dtm.matrix)
### cluster into 2 clusters
cl <- kmeans(m_norm, 2)
cl
cl$cluster
?weightTfIdf
### --- kNN Text Classification   ----
Doc1 <- "I spent 10K on my car. Compared to the prices of most cars in their class it was cheap. It is a red car so I like it and it has a lot of space."
Doc2 <- "I checked the car prices and I could not find a red car for under 10K. So the price was good even though it had a hole. I heard that it belonged to a movie star."
Doc3 <- "I like the red color, so I would buy a red car even if the car's price is over 10K."
Doc4 <- "I don't like red cars. The insurance for red cars is higher regardless of the price and I would not spend more than 10K. I like black cars."
Doc5 <- "A red giant star can curve the space to form a black hole. In absence of stars the space is flat."
Doc6 <- "With exception of the stars the space is filled with blackness making the black holes even harder to see."
Doc7 <- "Our sun is a small star and it will not end as a black hole. It does not have enough mass to curve the space."
Doc8 <- "Very few stars will end as black holes but still the space contains large number of black holes."
doc <- c(Doc1,Doc2,Doc3,Doc4,Doc5,Doc6,Doc7,Doc8)
corpus <- Corpus(VectorSource(doc))
# Preprocessing
corpus.temp <- tm_map(corpus, removePunctuation) # Remove Punctuation
corpus.temp <- tm_map(corpus.temp, stemDocument, language = "english")# Perform Stemming
dtm <- as.matrix(DocumentTermMatrix(corpus.temp)) # Document term matrix
# Text Classification
library(class) # Using kNN
train.doc <- dtm[c(1,2,5,6),] # Dataset for which classification is already known
test.doc <- dtm[c(3,4,7,8),] # Dataset you are trying to classify
Tags <- factor(c(rep("cars",2), rep("space",2))) # Tags - Correct answers for the training dataset
prob.test<- knn(train.doc, test.doc, Tags, k = 3, prob=TRUE) # k-number of neighbors considered
prob.test
# Display Classification Results
a <- c(3,4,7,8) #document ids
b <- prob.test #predicts by the algorithm
c <- attributes(prob.test)$prob #proportion of the votes for the winning class
c
result <- data.frame(Doc=a, Predict=b,Prob=c)
result
Tags
result
library(tm)#Framework for textmining.
library(SnowballC)#Provides wordStem() for stemming.
Doc1.Train.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-train/sci.space",sep=""))
Doc1.Train <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc1.Test.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-test/sci.space",sep=""))
Doc1.Test <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc2.Train.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-train/rec.autos",sep=""))
Doc2.Train <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc2.Test.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-test/rec.autos",sep=""))
Doc2.Test <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
# merge 4 corpus
merge.corpus <- c(Doc1.Train,Doc1.Test,Doc2.Train,Doc2.Test)
merge.corpus
#Preprocessing
# Convert to lower case
merge.corpus.tranf <- tm_map(merge.corpus, content_transformer(tolower))
# Remove Punctuation
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removePunctuation)
# Remove stop words
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removeWords, stopwords("english"))
# specify your stopwords as a character vector
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removeWords, c("since", "let", "yes", "every", "yeah"))
# ignore extremely rare words i.e. terms that appear in less then 1% of the documents
minTermFreq <- 5
dtm = DocumentTermMatrix(merge.corpus,
control = list(
minWordLength = 2, minDocFreq = 5
#wordLengths=c(2, Inf),
#bounds = list(global = c(minTermFreq, Inf))
))
dtm.matrix = as.matrix(dtm)
dtm.matrix[200:230, 20:30]
train.doc <- dtm.matrix[c(1:100,201:300),]
test.doc <- dtm.matrix[c(101:200,301:400),]
Tags <- factor(c(rep("Sci",100), rep("Rec",100)))
dim(train.doc)
dim(test.doc)
length(Tags)
library(class) # Using kNN
prob.test<- knn(train.doc, test.doc, Tags, k = 3, prob=TRUE)
prob.test
# Display Classification Results
a <- c(1:length(prob.test)) #document ids
b <- prob.test #predicts by the algorithm
a
b
#or b <- levels(prob.test)[prob.test]
c <- attributes(prob.test)$prob #proportion of the votes for the winning class
c
d <- prob.test == Tags
result <- data.frame(Doc=a, Predict=b,Prob=c,Correct= d)
result
#vinstall.packages("arules")
library("arules")
#create a sparse matrix
grocery <- read.transactions(".\\grocery.csv",  sep = ",")
summary(grocery)
#plot frequent items with min support = 0.1
itemFrequencyPlot(Groceries, support = 0.15)
#use apriori to generate rules
rules <- apriori(Groceries,
parameter = list(support = 0.006, confidence = 0.25, minlen = 2))
#get top five highest lift rules
inspect(sort(rules, by="lift")[1:5])
#find subset of the rules with berrries appearing in the rule
sub.rules <- subset(rules, rhs %in% "tropical fruit")
inspect(sub.rules)
sub.rules.2 <- subset(rules, items %in% c("berries", "yogurt") & lift > 3)
# or
sub.rules.2 <- subset(rules, items %in% c("berries", "yogurt") & lift > 3)
inspect(sub.rules.2)
sub.rules.2 <- subset(rules, (items %in% "berries" | items %in% "yogurt") & lift > 3)
# or
sub.rules.2 <- subset(rules, items %in% c("berries", "yogurt") & lift > 3)
inspect(sub.rules.2)
sub.rules.2
#vinstall.packages("arules")
library("arules")
#create a sparse matrix
grocery <- read.transactions(".\\grocery.csv",  sep = ",")
summary(grocery)
#plot frequent items with min support = 0.1
itemFrequencyPlot(Groceries, support = 0.15)
#use apriori to generate rules
rules <- apriori(Groceries,
parameter = list(support = 0.006, confidence = 0.25, minlen = 2))
#get top five highest lift rules
inspect(sort(rules, by="lift")[1:5])
#find subset of the rules with berrries appearing in the rule
sub.rules <- subset(rules, rhs %in% "tropical fruit")
inspect(sub.rules)
sub.rules.2 <- subset(rules, (items %in% "berries" | items %in% "yogurt") & lift > 3)
# or
sub.rules.2 <- subset(rules, items %in% c("berries", "yogurt") & lift > 3)
sub.rules.2
inspect(sub.rules.2)
iris
dim(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
head(iris)
#this command initializes R's random number generator to a specific sequence
#so set.seed to ensure reproducibility.
?set.seed
set.seed(20)
iris[, 3:4]
irisCluster <- kmeans(iris[, 3:4], 3)
irisCluster
#how good the clustering result?
table(irisCluster$cluster, iris$Species)
#plot the data to see the clusters
irisCluster$cluster.1 <- as.factor(irisCluster$cluster)
irisCluster$cluster.1
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster.1)) + geom_point()
#about between_SS and total_SS
irisCluster$centers
irisCluster$cluster
irisCluster$totss
irisCluster$withinss
mean1 <- mean(iris$Petal.Length)
mean2 <- mean(iris$Petal.Width)
#compute total_SS by hand
sum((iris$Petal.Length - mean1)^2 + (iris$Petal.Width - mean2)^2)
#compute within-cluster sum of squares for cluster 1
mean1 <- irisCluster$centers[1,1]
library(tm)#Framework for textmining.
library(SnowballC)#Provides wordStem() for stemming.
Doc1.Train.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-train/sci.space",sep=""))
Doc1.Train <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc1.Test.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-test/sci.space",sep=""))
Doc1.Test <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc2.Train.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-train/rec.autos",sep=""))
Doc2.Train <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
Doc2.Test.Source <- DirSource(paste(getwd(),"/DemoData/News2/20news-bydate-test/rec.autos",sep=""))
Doc2.Test <- Corpus(URISource(Doc1.Train.Source$filelist[1:100]), readerControl=list(reader=readPlain))
# merge 4 corpus
merge.corpus <- c(Doc1.Train,Doc1.Test,Doc2.Train,Doc2.Test)
merge.corpus
#Preprocessing
# Convert to lower case
merge.corpus.tranf <- tm_map(merge.corpus, content_transformer(tolower))
# Remove Punctuation
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removePunctuation)
# Remove stop words
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removeWords, stopwords("english"))
# specify your stopwords as a character vector
merge.corpus.tranf <- tm_map(merge.corpus.tranf, removeWords, c("since", "let", "yes", "every", "yeah"))
# ignore extremely rare words i.e. terms that appear in less then 1% of the documents
minTermFreq <- 5
dtm = DocumentTermMatrix(merge.corpus,
control = list(
minWordLength = 2, minDocFreq = 5
#wordLengths=c(2, Inf),
#bounds = list(global = c(minTermFreq, Inf))
))
dtm.matrix = as.matrix(dtm)
dtm.matrix[200:230, 20:30]
train.doc <- dtm.matrix[c(1:100,201:300),]
test.doc <- dtm.matrix[c(101:200,301:400),]
Tags <- factor(c(rep("Sci",100), rep("Rec",100)))
dim(train.doc)
dim(test.doc)
length(Tags)
library(class) # Using kNN
prob.test<- knn(train.doc, test.doc, Tags, k = 3, prob=TRUE)
prob.test
# Display Classification Results
a <- c(1:length(prob.test)) #document ids
b <- prob.test #predicts by the algorithm
#or b <- levels(prob.test)[prob.test]
c <- attributes(prob.test)$prob #proportion of the votes for the winning class
d <- prob.test == Tags
result <- data.frame(Doc=a, Predict=b,Prob=c,Correct= d)
result
#create a sparse matrix
grocery <- read.transactions(".\\grocery.csv",  sep = ",")
summary(grocery)
#plot frequent items with min support = 0.1
itemFrequencyPlot(Groceries, support = 0.15)
#use apriori to generate rules
rules <- apriori(Groceries,
parameter = list(support = 0.006, confidence = 0.25, minlen = 2))
#get top five highest lift rules
inspect(sort(rules, by="lift")[1:5])
#find subset of the rules with berrries appearing in the rule
sub.rules <- subset(rules, rhs %in% "tropical fruit")
inspect(sub.rules)
sub.rules.2 <- subset(rules, (items %in% "berries" | items %in% "yogurt") & lift > 3)
# or
sub.rules.2 <- subset(rules, items %in% c("berries", "yogurt") & lift > 3)
sub.rules.2
inspect(sub.rules.2)
iris
dim(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
head(iris)
#this command initializes R's random number generator to a specific sequence
#so set.seed to ensure reproducibility.
?set.seed
set.seed(20)
iris[, 3:4]
irisCluster <- kmeans(iris[, 3:4], 3)
?kmeans
irisCluster
#how good the clustering result?
table(irisCluster$cluster, iris$Species)
#plot the data to see the clusters
irisCluster$cluster.1 <- as.factor(irisCluster$cluster)
irisCluster$cluster.1
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster.1)) + geom_point()
#about between_SS and total_SS
irisCluster$centers
